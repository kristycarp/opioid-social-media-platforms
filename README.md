# Social Media for Opioid Trend Monitoring
**K.A. Carpenter, A.T. Nguyen, D.A. Smith, I.A. Samori, K. Humphreys, A. Lembke, M.V. Kiang, J.C. Eichstaedt, R.B. Altman**

This repository contains all code and data associated with the paper, "Which social media platforms facilitate monitoring the opioid crisis?" *(in review)*. A preprint is available [here](https://www.medrxiv.org/content/10.1101/2024.07.06.24310035v2).

We organize the scripts and data according to what stage of the study they are associated with. For more detailed specifications of each csv, please see `data_dictionaries.md`.

1. Filtering platform list to shortlist
- **`platform_list.csv`** - This CSV is a dataframe containing the 72 platforms assessed in the study, their respective URLs, and whether they fulfill each of our shortlisting criteria described in the paper.
- **`filter_platform_list.R`** - An R script that filters the platform list from `platform_list.csv` according to the shortlisting criteria and creates a visualization of platform attrition.
- **`filtered_platforms.csv`** - This CSV is the output dataframe resulting from `filter_platform_list.R`, showing which platforms were kept in the shortlist and at which criterion the non-shortlisted platforms were dropped, as well as the number of Google search hits for the formal opioid term list.

2. Creating set of household and formal terms
- **`formal-terms.csv`** - The list of 9 "formal" opioid terms used in this analysis.
- **`household-terms.csv`** - The list of 9 "household" terms used in this analysis to normalize the number of opioid hits (see manuscript for more details).

2. Creating set of informal terms
- **`gpt3/*csv`** - The CSVs contained within the `gpt3` directory originate from the [Build colloquial lexicons with GPT-3 repository](https://github.com/kristycarp/gpt3-lexicon). They contain GPT-3-generated colloquial synonyms (slang, misspellings, etc.) of opioids, along with annotations of whether the generated term is found in the [RedMed](https://github.com/alavertu/redmed) lexicon and whether the generated term is validated by automated Google queries (see the [paper](https://www.mdpi.com/2218-273X/13/2/387) for more details).
- **`informal_with_freqs.csv`** - This CSV is a dataframe of informal slang terms and misspellings, generated by GPT-3, and filtered for specificity (see manuscript for details). The frequency with which each term was generated by GPT-3 is included so as to create a set of informal terms comparable in length to the other term lists.

3. Creating set of algospeak terms
- **`algospeak/query.py`** - Python script to query GPT-4 for algospeak terms for a given drug. Usage: `python algospeak/query.py --drug [DRUG NAME] -n [NUM QUERY REPETITIONS]`
- **`algospeak/*-output.txt`** - Output from `algospeak/query.py` containing raw GPT-4 responses to the query.
- **`algospeak/process_output.py`** - Python script to process the raw GPT-4 response output from `algospeak/query.py`. Usage: `python algospeak/process_output.py --drug [DRUG NAME]`
- **`algospeak/*-error-out.txt`** - Output from `algospeak/process_output.py` containing any lines for which there were parsing errors.
- **`algospeak/*-processed-out.txt`** - Output from `algospeak/process_output.py` containing only the GPT-4-generated algospeak terms for the specified drug.
- **`algospeak/*-terms.csv`** - Dataframe containing all unique algospeak terms generated by GPT-4 for the specified drug, along with the total number of times each term was generated. 
- **`all-algospeak-terms.csv`** - Algospeak terms for all queried drugs, filtered to only include terms with special characters (e.g. including terms like *f3nny* but not *fenny*), along with the frequency of generation and whether or not to include the term in the final algospeak term list.

4. Querying Google for term hits on given platform
- **`search_queries.R`** - R script that uses the Google Search API to obtain term hit results on a given website. Note that to use this script, you will need to make your own API key and store it in a `.env` file.
- **`query_outputs/response*.RDS`** - R Data Serialization files generated by `search_queries.R` containing the Google Search API responses for a given term on a given platform website, so as to prevent wasting API calls on redundant queries. Note that in order to refresh the Google Search hit results, the corresponding RDS file will need to be deleted.
- **`google_search_results_*.csv`** - Total combined number of hits for each term on the given list (e.g., `google_search_results_algospeak.csv` contains the total combined number of hits for each term on the algospeak terms list). The results were obtained by running `search_queries.R`.

5. Make plots
- **`scatterplots.R`** - R script to make scatterplots of formal, informal, and algospeak hits for each platform (as shown in Figures 2 and 3 in the manuscript)
